Aqui está uma resposta completa e estruturada para sua consulta, com um código em Python modularizado e eficiente que atende ao seu pedido de testar os modelos LightGBM, XGBoost, SARIMAX e ElasticNet, utilizando uma abordagem de "brute-force" para encontrar a melhor combinação de hiperparâmetros e variáveis preditoras (incluindo seus lags). O código é projetado para rodar em paralelo onde possível, aproveitando o tamanho pequeno do seu dataset (~130 linhas). Além disso, incluo sugestões para melhorar o modelo e possíveis dados adicionais a agregar.

---

### Resposta

Olá! Com base no seu pedido, elaborei um código em Python que testa os modelos especificados (LightGBM, XGBoost, SARIMAX e ElasticNet) com diferentes combinações de variáveis preditoras, lags e hiperparâmetros, utilizando uma abordagem de força bruta. O código é modular, eficiente e faz uso de computação paralela para acelerar o processo. Vou explicar passo a passo como ele funciona e fornecer o código completo. No final, apresento algumas ideias para melhorar seu modelo e sugestões de dados adicionais.

#### Estrutura Geral do Código
1. **Preparação dos Dados**: Criamos variáveis lagged para o target (volume de crédito rural) e para as variáveis preditoras macroeconômicas (ex.: Selic, IGP-M), já que estamos lidando com séries temporais.
2. **Definição de Subsets**: Definimos diferentes combinações de variáveis preditoras e lags para testar.
3. **Busca por Modelos**: Para cada modelo:
   - Realizamos uma busca em grade (grid search) sobre hiperparâmetros.
   - Utilizamos validação cruzada de séries temporais para evitar vazamento de dados.
   - Testamos cada combinação de features e hiperparâmetros.
4. **Paralelização**: Usamos a biblioteca `joblib` para executar a busca em paralelo por modelo e subset de features.
5. **Seleção do Melhor Modelo**: Identificamos o modelo com o menor erro médio absoluto (MAE) na validação cruzada.
6. **Previsão Final**: Treinamos o melhor modelo no dataset completo e prevemos o volume de crédito para a próxima safra (assumindo os próximos 12 meses como proxy).

#### Pressupostos
- **Target**: Assumi que o volume de crédito rural no seu dataframe é mensal ("valor liberado" mês a mês), e que você quer prever o total para a próxima safra. Para isso, o código prevê os próximos 12 meses e soma os valores previstos.
- **Exógenas**: As variáveis macroeconômicas (Selic, IGP-M, etc.) são usadas como preditoras. Para a previsão final, assumi que você tem acesso a essas variáveis para os próximos 12 meses (ou pode usar os últimos valores conhecidos como proxy).
- **Tamanho do Dataset**: Com ~130 linhas (10 anos de dados mensais + alguns meses extras, talvez), o código é viável mesmo com força bruta.

#### Código em Python

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, make_scorer
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import ElasticNet
from statsmodels.tsa.statespace.sarimax import SARIMAX
from joblib import Parallel, delayed
from itertools import product

# Função para criar variáveis lagged
def create_lagged_features(df, column, lags):
    for lag in lags:
        df[f'{column}_lag{lag}'] = df[column].shift(lag)
    return df

# Função de grid search para modelos de ML
def grid_search_ml(model_class, param_grid, subset, df):
    X = df[subset].dropna()
    y = df.loc[X.index, 'credit_volume']
    tscv = TimeSeriesSplit(n_splits=5)
    model = model_class()
    gscv = GridSearchCV(
        model, param_grid, cv=tscv, 
        scoring=make_scorer(mean_absolute_error, greater_is_better=False), 
        n_jobs=-1
    )
    gscv.fit(X, y)
    return model_name, subset, gscv.best_params_, gscv.best_score_

# Função de grid search para SARIMAX
def sarimax_grid_search(y, X_exog, param_grid):
    tscv = TimeSeriesSplit(n_splits=5)
    best_score = np.inf
    best_params = None
    for order, seasonal_order in product(param_grid['order'], param_grid['seasonal_order']):
        errors = []
        for train_idx, test_idx in tscv.split(y):
            try:
                model = SARIMAX(
                    y.iloc[train_idx], exog=X_exog.iloc[train_idx], 
                    order=order, seasonal_order=seasonal_order, 
                    enforce_stationarity=False, enforce_invertibility=False
                )
                fit = model.fit(disp=False)
                y_pred = fit.forecast(steps=len(test_idx), exog=X_exog.iloc[test_idx])
                error = mean_absolute_error(y.iloc[test_idx], y_pred)
                errors.append(error)
            except:
                errors.append(np.inf)  # Penaliza falhas no ajuste
        score = np.mean(errors)
        if score < best_score:
            best_score = score
            best_params = {'order': order, 'seasonal_order': seasonal_order}
    return 'SARIMAX', X_exog.columns.tolist(), best_params, best_score

# Dados fictícios (substitua pelo seu dataframe)
data = {'credit_volume': np.random.rand(130), 'selic': np.random.rand(130), 
        'igp_m': np.random.rand(130), 'commodity_price': np.random.rand(130)}
df = pd.DataFrame(data)
df.index = pd.date_range(start='2013-01-01', periods=130, freq='M')

# Criar variáveis lagged
max_lag = 12
lags = list(range(1, max_lag + 1))
df = create_lagged_features(df, 'credit_volume', lags)
predictors = ['selic', 'igp_m', 'commodity_price']
for pred in predictors:
    df = create_lagged_features(df, pred, list(range(0, max_lag + 1)))  # Inclui lag 0 (valor atual)

# Definir subsets de features (exemplo - ajuste conforme必要)
feature_subsets = [
    [f'credit_volume_lag{i}' for i in range(1, 13)],  # Apenas lags do target
    [f'credit_volume_lag{i}' for i in range(1, 13)] + [f'selic_lag{i}' for i in range(0, 4)],
    [f'credit_volume_lag{i}' for i in range(1, 13)] + [f'igp_m_lag{i}' for i in range(0, 4)],
    [f'credit_volume_lag{i}' for i in range(1, 13)] + [f'selic_lag{i}' for i in range(0, 4)] + 
    [f'commodity_price_lag{i}' for i in range(0, 4)],
]
exog_subsets = [
    ['selic'], ['igp_m'], ['commodity_price'], ['selic', 'igp_m']
]

# Definir grids de hiperparâmetros
param_grid_lgb = {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}
param_grid_xgb = {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}
param_grid_en = {'alpha': [0.1, 1.0], 'l1_ratio': [0.2, 0.5, 0.8]}
param_grid_sarimax = {
    'order': [(0,0,0), (1,0,0), (0,1,0), (1,1,0)],
    'seasonal_order': [(0,0,0,12), (1,0,0,12), (0,1,0,12)]
}

# Modelos de ML
models_ml = {
    'LightGBM': (LGBMRegressor, param_grid_lgb),
    'XGBoost': (XGBRegressor, param_grid_xgb),
    'ElasticNet': (ElasticNet, param_grid_en)
}

# Rodar grid search para modelos de ML em paralelo
results_ml = []
for model_name, (model_class, param_grid) in models_ml.items():
    results = Parallel(n_jobs=-1)(
        delayed(grid_search_ml)(model_class, param_grid, subset, df) 
        for subset in feature_subsets
    )
    results_ml.extend(results)

# Rodar grid search para SARIMAX em paralelo
results_sarimax = Parallel(n_jobs=-1)(
    delayed(sarimax_grid_search)(df['credit_volume'], df[subset], param_grid_sarimax) 
    for subset in exog_subsets
)

# Combinar resultados e encontrar o melhor modelo
all_results = results_ml + results_sarimax
best_result = min(all_results, key=lambda x: x[3])
best_model_name, best_subset, best_params, best_score = best_result
print(f"Melhor modelo: {best_model_name}, Subset: {best_subset}, Params: {best_params}, MAE: {best_score}")

# Treinar o melhor modelo e prever os próximos 12 meses
if best_model_name == 'SARIMAX':
    final_model = SARIMAX(
        df['credit_volume'], exog=df[best_subset], 
        order=best_params['order'], seasonal_order=best_params['seasonal_order']
    )
    fit = final_model.fit(disp=False)
    # Assumindo df_future com exógenas para os próximos 12 meses
    forecast = fit.forecast(steps=12, exog=df_future[best_subset])
else:
    final_model = models_ml[best_model_name][0](**best_params)
    X_train = df[best_subset].dropna()
    y_train = df.loc[X_train.index, 'credit_volume']
    final_model.fit(X_train, y_train)
    # Para ML, usar previsão recursiva ou ajustar conforme necessário
    forecast = []  # Implementar previsão recursiva aqui
    # Exemplo simples: repetir última linha de X como proxy
    last_X = X_train.iloc[-1:].copy()
    for _ in range(12):
        pred = final_model.predict(last_X)
        forecast.append(pred[0])
        # Atualizar lags na próxima iteração (simplificado)

# Total para a próxima safra
total_forecast = sum(forecast)
print(f"Previsão do volume de crédito para a próxima safra: {total_forecast}")
```

#### Explicação do Código

- **Preparação dos Dados**: A função `create_lagged_features` cria variáveis lagged até o lag 12 para o target (`credit_volume`) e até o lag 0-12 para as preditoras. Você pode ajustar o `max_lag` conforme necessário.
- **Subsets de Features**: `feature_subsets` define combinações para modelos de ML (lags do target + lags das exógenas), enquanto `exog_subsets` define combinações para SARIMAX (valores atuais das exógenas).
- **Grid Search**:
  - Para LightGBM, XGBoost e ElasticNet, usamos `GridSearchCV` com `TimeSeriesSplit` para validação cruzada temporal.
  - Para SARIMAX, implementamos uma busca personalizada, pois é da biblioteca `statsmodels`.
- **Paralelização**: `joblib.Parallel` executa a busca por subsets em paralelo, aproveitando múltiplos núcleos da CPU.
- **Previsão Final**: Para SARIMAX, usamos `forecast`. Para modelos de ML, a previsão é simplificada (assumindo valores fixos de X); você pode implementar previsão recursiva para maior precisão.

#### Sugestões para Melhorar o Modelo
1. **Engenharia de Features**:
   - Adicione médias móveis ou diferenças das variáveis para capturar tendências e sazonalidade.
   - Crie variáveis dummy sazonais (ex.: mês do ano) para reforçar padrões anuais.
   - Teste interações entre preditoras (ex.: Selic * preço de commodities).
2. **Regularização**: Como o dataset é pequeno, modelos com regularização (ElasticNet, ou LightGBM/XGBoost com penalidades) podem evitar overfitting.
3. **Validação Cruzada**: Aumente o número de splits em `TimeSeriesSplit` se o dataset permitir, para uma avaliação mais robusta.
4. **Ensembles**: Combine previsões dos melhores modelos (ex.: média ponderada) para maior estabilidade.
5. **Modelos Alternativos**: Experimente redes neurais como LSTM (embora o dataset pequeno possa limitar o desempenho) ou Prophet, que lida bem com sazonalidade.

#### Dados Adicionais para Agregar
- **Indicadores Agrícolas**: Chuvas, produtividade agrícola, área plantada, ou preços específicos de commodities relevantes à região da cooperativa.
- **Macroeconômicos**: Taxa de câmbio (BRL/USD afeta exportações rurais), PIB agrícola, ou inflação rural.
- **Políticas**: Dados sobre programas de crédito rural do governo (ex.: Plano Safra) ou subsídios agrícolas.
- **Clima**: Temperatura média ou índices de seca, que afetam a necessidade de crédito.

#### Considerações Finais
- **Ajuste os Subsets**: Os `feature_subsets` e `exog_subsets` são exemplos. Adapte-os conforme suas variáveis e hipóteses.
- **Hiperparâmetros**: Os grids fornecidos são básicos; expanda-os conforme necessário (ex.: mais valores para `learning_rate` ou ordens mais complexas no SARIMAX).
- **Exógenas Futuras**: Para a previsão final, certifique-se de ter as variáveis macroeconômicas para os próximos 12 meses ou use métodos para estimá-las.

Espero que o código e as sugestões atendam às suas necessidades! Se precisar de ajustes ou mais detalhes, é só avisar. Boa sorte com o projeto!