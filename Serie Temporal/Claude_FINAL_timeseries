import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import lightgbm as lgb

# Para ARIMA e SARIMAX
try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.tsa.stattools import adfuller
    from itertools import product
    HAS_STATSMODELS = True
except ImportError:
    print("Aviso: statsmodels não encontrado. ARIMA e SARIMAX não estarão disponíveis.")
    HAS_STATSMODELS = False

warnings.filterwarnings('ignore')

def time_series_forecast_comparison(df, target_col, date_col=None, 
                                  test_size=0.2, n_lags=12, 
                                  external_features=None,
                                  seasonal_periods=None,
                                  algorithms=None,
                                  cv_folds=3,
                                  random_state=42,
                                  verbose=True):
    """
    Compara múltiplos algoritmos de machine learning para forecast de séries temporais.
    
    Parâmetros:
    -----------
    df : pandas.DataFrame
        DataFrame com os dados da série temporal
    target_col : str
        Nome da coluna target (variável a ser prevista)
    date_col : str, optional
        Nome da coluna de data. Se None, assume que o index é temporal
    test_size : float, default=0.2
        Proporção dos dados para teste (validação final)
    n_lags : int, default=12
        Número de lags a serem criados
    external_features : list, optional
        Lista de features externas (exógenas) a serem incluídas
    seasonal_periods : int, optional
        Período sazonal (12 para mensal, 4 para trimestral, etc.)
    algorithms : list, optional
        Lista de algoritmos a testar. Se None, testa todos disponíveis
    cv_folds : int, default=3
        Número de folds para TimeSeriesSplit na validação cruzada
    random_state : int, default=42
        Seed para reprodutibilidade  
    verbose : bool, default=True
        Se True, mostra progresso detalhado
        
    Retorna:
    --------
    dict : {
        'results': DataFrame com métricas de todos os modelos,
        'best_model': nome do melhor modelo,
        'models': dict com os modelos treinados,
        'predictions': dict com as previsões de cada modelo,
        'data_info': dict com informações dos dados processados
    }
    """
    
    # Algoritmos disponíveis
    available_algorithms = ['ElasticNet', 'XGBoost', 'XGBoost_Linear', 'LightGBM', 'LightGBM_Linear']
    if HAS_STATSMODELS:
        available_algorithms.extend(['ARIMA', 'SARIMAX'])
    
    if algorithms is None:
        algorithms = available_algorithms
    else:
        # Verificar disponibilidade
        if not HAS_STATSMODELS:
            algorithms = [alg for alg in algorithms if alg not in ['ARIMA', 'SARIMAX']]
            if verbose:
                print("Aviso: ARIMA e SARIMAX removidos (statsmodels não disponível)")
    
    if verbose:
        print("=== COMPARAÇÃO DE ALGORITMOS PARA FORECAST ===")
        print(f"Target: {target_col}")
        print(f"Algoritmos a testar: {algorithms}")
        print(f"Número de lags: {n_lags}")
        print(f"Tamanho do teste: {test_size}")
        print(f"CV folds: {cv_folds}")
        print("-" * 60)
    
    # Preparar dados
    df_work = df.copy()
    
    # Configurar index temporal
    if date_col is not None:
        if date_col in df_work.columns:
            df_work[date_col] = pd.to_datetime(df_work[date_col])
            df_work = df_work.set_index(date_col)
        else:
            raise ValueError(f"Coluna de data '{date_col}' não encontrada")
    
    # Verificar se index é temporal
    if not isinstance(df_work.index, pd.DatetimeIndex):
        if verbose:
            print("Aviso: Index não é temporal. Criando index sequencial...")
        df_work.index = pd.date_range(start='2020-01-01', periods=len(df_work), freq='M')
    
    # Ordenar por data
    df_work = df_work.sort_index()
    
    if verbose:
        print(f"Período dos dados: {df_work.index.min()} até {df_work.index.max()}")
        print(f"Frequência detectada: {pd.infer_freq(df_work.index)}")
    
    # Verificar target
    if target_col not in df_work.columns:
        raise ValueError(f"Coluna target '{target_col}' não encontrada")
    
    # Análise básica da série
    target_series = df_work[target_col].dropna()
    
    if len(target_series) < n_lags + 10:
        raise ValueError(f"Série muito curta. Mínimo necessário: {n_lags + 10} observações")
    
    if verbose:
        print(f"Observações válidas: {len(target_series)}")
        print(f"Média: {target_series.mean():.2f}")
        print(f"Desvio padrão: {target_series.std():.2f}")
    
    # 1. CRIAR LAGS E FEATURES
    if verbose:
        print(f"\n--- CRIANDO {n_lags} LAGS ---")
    
    # DataFrame para features
    features_df = pd.DataFrame(index=df_work.index)
    features_df['target'] = df_work[target_col]
    
    # Criar lags do target
    for lag in range(1, n_lags + 1):
        features_df[f'lag_{lag}'] = features_df['target'].shift(lag)
    
    # Adicionar features externas se fornecidas
    if external_features:
        for feature in external_features:
            if feature in df_work.columns:
                features_df[feature] = df_work[feature]
                # Criar lags das features externas também
                for lag in range(1, min(6, n_lags + 1)):  # Menos lags para externas
                    features_df[f'{feature}_lag_{lag}'] = df_work[feature].shift(lag)
    
    # Features de tendência e sazonalidade
    features_df['trend'] = np.arange(len(features_df))
    features_df['month'] = features_df.index.month
    features_df['quarter'] = features_df.index.quarter
    features_df['year'] = features_df.index.year
    
    # Médias móveis
    for window in [3, 6, 12]:
        if window <= len(target_series):
            features_df[f'ma_{window}'] = features_df['target'].rolling(window=window).mean()
    
    # Remover NaN
    features_df = features_df.dropna()
    
    if verbose:
        print(f"Features criadas: {len(features_df.columns) - 1}")  # -1 para excluir target
        print(f"Observações após criar lags: {len(features_df)}")
    
    # 2. DIVISÃO TEMPORAL DOS DADOS
    n_total = len(features_df)
    n_test = int(n_total * test_size)
    n_train = n_total - n_test
    
    # Separar features e target
    feature_cols = [col for col in features_df.columns if col != 'target']
    X = features_df[feature_cols]
    y = features_df['target']
    
    # Divisão temporal
    X_train, X_test = X.iloc[:n_train], X.iloc[n_train:]
    y_train, y_test = y.iloc[:n_train], y.iloc[n_train:]
    
    if verbose:
        print(f"\n--- DIVISÃO DOS DADOS ---")
        print(f"Treino: {len(X_train)} obs ({X_train.index.min()} até {X_train.index.max()})")
        print(f"Teste: {len(X_test)} obs ({X_test.index.min()} até {X_test.index.max()})")
    
    # 3. CONFIGURAR TIME SERIES SPLIT PARA CV
    tscv = TimeSeriesSplit(n_splits=cv_folds)
    
    # 4. DICIONÁRIOS PARA ARMAZENAR RESULTADOS
    results = []
    trained_models = {}
    predictions = {}
    
    # 5. TESTAR CADA ALGORITMO
    for algorithm in algorithms:
        if verbose:
            print(f"\n{'='*20} {algorithm} {'='*20}")
        
        try:
            if algorithm == 'ElasticNet':
                model, pred = _train_elasticnet(X_train, y_train, X_test, y_test, tscv, verbose)
            
            elif algorithm == 'XGBoost':
                model, pred = _train_xgboost(X_train, y_train, X_test, y_test, tscv, verbose, booster='gbtree')
            
            elif algorithm == 'XGBoost_Linear':
                model, pred = _train_xgboost(X_train, y_train, X_test, y_test, tscv, verbose, booster='gblinear')
            
            elif algorithm == 'LightGBM':
                model, pred = _train_lightgbm(X_train, y_train, X_test, y_test, tscv, verbose, boosting_type='gbdt')
            
            elif algorithm == 'LightGBM_Linear':
                model, pred = _train_lightgbm(X_train, y_train, X_test, y_test, tscv, verbose, boosting_type='dart')
            
            elif algorithm == 'ARIMA' and HAS_STATSMODELS:
                model, pred = _train_arima(y_train, y_test, seasonal_periods, verbose)
            
            elif algorithm == 'SARIMAX' and HAS_STATSMODELS:
                exog_train = X_train[external_features] if external_features else None
                exog_test = X_test[external_features] if external_features else None
                model, pred = _train_sarimax(y_train, y_test, exog_train, exog_test, seasonal_periods, verbose)
            
            else:
                if verbose:
                    print(f"Algoritmo {algorithm} não disponível ou não implementado")
                continue
            
            # Calcular métricas
            mae = mean_absolute_error(y_test, pred)
            mse = mean_squared_error(y_test, pred)
            rmse = np.sqrt(mse)
            mape = mean_absolute_percentage_error(y_test, pred) * 100
            
            # Métricas direcionais
            direction_accuracy = _calculate_direction_accuracy(y_test, pred)
            
            # Armazenar resultados
            results.append({
                'Algorithm': algorithm,
                'MAE': mae,
                'MSE': mse,
                'RMSE': rmse,
                'MAPE': mape,
                'Direction_Accuracy': direction_accuracy,
                'R2_Score': 1 - (mse / np.var(y_test))
            })
            
            trained_models[algorithm] = model
            predictions[algorithm] = pred
            
            if verbose:
                print(f"✅ {algorithm} - RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, Direction: {direction_accuracy:.2f}%")
        
        except Exception as e:
            if verbose:
                print(f"❌ Erro em {algorithm}: {str(e)}")
            continue
    
    # 6. COMPILAR RESULTADOS
    results_df = pd.DataFrame(results)
    
    if len(results_df) == 0:
        raise ValueError("Nenhum modelo foi treinado com sucesso")
    
    # Ordenar por RMSE (menor é melhor)
    results_df = results_df.sort_values('RMSE').reset_index(drop=True)
    best_model_name = results_df.iloc[0]['Algorithm']
    
    if verbose:
        print(f"\n{'='*60}")
        print("=== RESULTADOS FINAIS ===")
        print(results_df.round(4))
        print(f"\n🏆 MELHOR MODELO: {best_model_name}")
        print(f"   RMSE: {results_df.iloc[0]['RMSE']:.4f}")
        print(f"   MAPE: {results_df.iloc[0]['MAPE']:.2f}%")
    
    # Informações sobre os dados processados
    data_info = {
        'n_total_obs': n_total,
        'n_train_obs': n_train,
        'n_test_obs': n_test,
        'n_features': len(feature_cols),
        'feature_names': feature_cols,
        'date_range': (features_df.index.min(), features_df.index.max()),
        'target_stats': {
            'mean': float(y.mean()),
            'std': float(y.std()),
            'min': float(y.min()),
            'max': float(y.max())
        }
    }
    
    return {
        'results': results_df,
        'best_model': best_model_name,
        'models': trained_models,
        'predictions': predictions,
        'data_info': data_info,
        'test_data': {'y_true': y_test, 'X_test': X_test}
    }


# FUNÇÕES AUXILIARES PARA CADA ALGORITMO

def _train_elasticnet(X_train, y_train, X_test, y_test, tscv, verbose):
    """Treina ElasticNet com grid search"""
    
    # Normalizar features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Grid de hiperparâmetros
    param_grid = {
        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],
        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]
    }
    
    # Grid search com time series CV
    elastic_net = ElasticNet(random_state=42, max_iter=1000)
    grid_search = GridSearchCV(
        elastic_net, param_grid, cv=tscv, 
        scoring='neg_mean_squared_error', n_jobs=-1
    )
    
    grid_search.fit(X_train_scaled, y_train)
    
    if verbose:
        print(f"Melhores params: {grid_search.best_params_}")
    
    # Previsão
    pred = grid_search.predict(X_test_scaled)
    
    return grid_search, pred


def _train_xgboost(X_train, y_train, X_test, y_test, tscv, verbose, booster='gbtree'):
    """Treina XGBoost com grid search e suporte a booster linear"""
    
    if booster == 'gbtree':
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0],
            'reg_alpha': [0, 0.1, 1],  # L1 regularization
            'reg_lambda': [1, 1.5, 2]  # L2 regularization
        }
    else:  # gblinear
        param_grid = {
            'n_estimators': [100, 200, 500],
            'learning_rate': [0.01, 0.1, 0.3],
            'reg_alpha': [0, 0.1, 0.5, 1.0],  # L1 regularization
            'reg_lambda': [0.1, 1.0, 2.0, 5.0],  # L2 regularization
            'feature_selector': ['cyclic', 'shuffle']  # Feature selection for linear booster
        }
    
    xgb_model = xgb.XGBRegressor(
        booster=booster, 
        random_state=42, 
        n_jobs=-1,
        tree_method='hist' if booster == 'gbtree' else None
    )
    
    grid_search = GridSearchCV(
        xgb_model, param_grid, cv=tscv,
        scoring='neg_mean_squared_error', n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    if verbose:
        print(f"Booster: {booster}")
        print(f"Melhores params: {grid_search.best_params_}")
    
    pred = grid_search.predict(X_test)
    
    return grid_search, pred


def _train_lightgbm(X_train, y_train, X_test, y_test, tscv, verbose, boosting_type='gbdt'):
    """Treina LightGBM com grid search e diferentes boosting types"""
    
    if boosting_type == 'gbdt':
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7, -1],  # -1 significa sem limite
            'learning_rate': [0.01, 0.1, 0.2],
            'num_leaves': [15, 31, 63],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0],
            'reg_alpha': [0, 0.1, 1],  # L1 regularization
            'reg_lambda': [0, 0.1, 1],  # L2 regularization
            'min_child_samples': [10, 20, 30]
        }
    else:  # dart (Dropouts meet Multiple Additive Regression Trees)
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.2],
            'num_leaves': [15, 31, 63],
            'drop_rate': [0.1, 0.15, 0.2],  # Dropout rate for DART
            'skip_drop': [0.3, 0.5, 0.7],   # Probability of skipping dropout
            'reg_alpha': [0, 0.1, 1],
            'reg_lambda': [0, 0.1, 1]
        }
    
    lgb_model = lgb.LGBMRegressor(
        boosting_type=boosting_type,
        random_state=42, 
        n_jobs=-1, 
        verbose=-1,
        force_col_wise=True  # Otimização para datasets pequenos
    )
    
    grid_search = GridSearchCV(
        lgb_model, param_grid, cv=tscv,
        scoring='neg_mean_squared_error', n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    if verbose:
        print(f"Boosting type: {boosting_type}")
        print(f"Melhores params: {grid_search.best_params_}")
    
    pred = grid_search.predict(X_test)
    
    return grid_search, pred


def _train_arima(y_train, y_test, seasonal_periods, verbose):
    """Treina ARIMA com busca automática de parâmetros"""
    
    if not HAS_STATSMODELS:
        raise ImportError("statsmodels não está disponível")
    
    # Testar diferentes configurações ARIMA
    best_aic = float('inf')
    best_model = None
    best_order = None
    
    # Grid de parâmetros ARIMA (p, d, q)
    p_values = range(0, 4)
    d_values = range(0, 3)
    q_values = range(0, 4)
    
    for p in p_values:
        for d in d_values:
            for q in q_values:
                try:
                    model = ARIMA(y_train, order=(p, d, q))
                    fitted_model = model.fit()
                    
                    if fitted_model.aic < best_aic:
                        best_aic = fitted_model.aic
                        best_model = fitted_model
                        best_order = (p, d, q)
                        
                except:
                    continue
    
    if best_model is None:
        # Fallback para ARIMA simples
        best_model = ARIMA(y_train, order=(1, 1, 1)).fit()
        best_order = (1, 1, 1)
    
    if verbose:
        print(f"Melhor ordem ARIMA: {best_order}, AIC: {best_aic:.2f}")
    
    # Previsão
    forecast = best_model.forecast(steps=len(y_test))
    
    return best_model, forecast


def _train_sarimax(y_train, y_test, exog_train, exog_test, seasonal_periods, verbose):
    """Treina SARIMAX com busca extensiva de hiperparâmetros"""
    
    if not HAS_STATSMODELS:
        raise ImportError("statsmodels não está disponível")
    
    # Determinar sazonalidade
    if seasonal_periods is None:
        seasonal_periods = 12  # Default mensal
    
    # Grid de parâmetros para busca extensiva
    p_values = range(0, 3)
    d_values = range(0, 2)
    q_values = range(0, 3)
    P_values = range(0, 2)  # Parâmetros sazonais
    D_values = range(0, 2)
    Q_values = range(0, 2)
    
    best_aic = float('inf')
    best_model = None
    best_order = None
    best_seasonal_order = None
    
    total_combinations = len(p_values) * len(d_values) * len(q_values) * len(P_values) * len(D_values) * len(Q_values)
    
    if verbose:
        print(f"Testando {total_combinations} combinações de parâmetros SARIMAX...")
    
    tested_count = 0
    
    for p, d, q in product(p_values, d_values, q_values):
        for P, D, Q in product(P_values, D_values, Q_values):
            try:
                # Evitar configurações muito complexas
                if p + d + q + P + D + Q > 6:
                    continue
                    
                model = SARIMAX(
                    y_train,
                    exog=exog_train,
                    order=(p, d, q),
                    seasonal_order=(P, D, Q, seasonal_periods),
                    enforce_stationarity=False,
                    enforce_invertibility=False,
                    simple_differencing=True  # Melhora performance
                )
                
                fitted_model = model.fit(
                    disp=False, 
                    maxiter=100,  # Limitar iterações para velocidade
                    method='lbfgs'  # Método de otimização mais rápido
                )
                
                if fitted_model.aic < best_aic:
                    best_aic = fitted_model.aic
                    best_model = fitted_model
                    best_order = (p, d, q)
                    best_seasonal_order = (P, D, Q, seasonal_periods)
                
                tested_count += 1
                
                # Progress feedback
                if verbose and tested_count % 20 == 0:
                    print(f"Testado {tested_count} modelos. Melhor AIC até agora: {best_aic:.2f}")
                    
            except Exception as e:
                continue
    
    # Se nenhum modelo funcionou, usar configuração simples
    if best_model is None:
        try:
            model = SARIMAX(
                y_train,
                exog=exog_train,
                order=(1, 1, 1),
                seasonal_order=(1, 1, 1, seasonal_periods),
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            best_model = model.fit(disp=False)
            best_order = (1, 1, 1)
            best_seasonal_order = (1, 1, 1, seasonal_periods)
            best_aic = best_model.aic
            
        except Exception as e:
            # Último fallback: ARIMA simples
            if verbose:
                print(f"SARIMAX falhou completamente, usando ARIMA: {e}")
            return _train_arima(y_train, y_test, seasonal_periods, verbose)
    
    if verbose:
        print(f"Melhor configuração SARIMAX:")
        print(f"  Ordem: {best_order}")
        print(f"  Ordem sazonal: {best_seasonal_order}")
        print(f"  AIC: {best_aic:.2f}")
        print(f"  Modelos testados: {tested_count}")
    
    # Previsão
    try:
        forecast = best_model.forecast(steps=len(y_test), exog=exog_test)
    except:
        # Se falhar na previsão, usar get_forecast
        forecast_result = best_model.get_forecast(steps=len(y_test), exog=exog_test)
        forecast = forecast_result.predicted_mean
    
    return best_model, forecast


def _calculate_direction_accuracy(y_true, y_pred):
    """Calcula acurácia direcional (se previu corretamente subida/descida)"""
    
    if len(y_true) < 2:
        return 0.0
    
    true_direction = np.diff(y_true) > 0
    pred_direction = np.diff(y_pred) > 0
    
    return np.mean(true_direction == pred_direction) * 100


def plot_forecast_comparison(comparison_results):
    """Visualiza os resultados da comparação"""
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        results_df = comparison_results['results']
        predictions = comparison_results['predictions']
        y_test = comparison_results['test_data']['y_true']
        
        # Figura com múltiplos subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Comparação de métricas
        metrics = ['RMSE', 'MAPE', 'Direction_Accuracy']
        x = np.arange(len(results_df))
        width = 0.25
        
        for i, metric in enumerate(metrics):
            ax1.bar(x + i * width, results_df[metric], width, 
                   label=metric, alpha=0.8)
        
        ax1.set_xlabel('Algoritmos')
        ax1.set_ylabel('Valor da Métrica')
        ax1.set_title('Comparação de Métricas por Algoritmo')
        ax1.set_xticks(x + width)
        ax1.set_xticklabels(results_df['Algorithm'], rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Ranking de modelos (RMSE)
        colors = ['gold', 'silver', '#CD7F32'] + ['lightblue'] * (len(results_df) - 3)
        ax2.barh(range(len(results_df)), results_df['RMSE'], 
                color=colors[:len(results_df)])
        ax2.set_yticks(range(len(results_df)))
        ax2.set_yticklabels(results_df['Algorithm'])
        ax2.set_xlabel('RMSE')
        ax2.set_title('Ranking dos Modelos (RMSE)')
        ax2.grid(True, alpha=0.3)
        
        # 3. Previsões vs Real (melhor modelo)
        best_model = results_df.iloc[0]['Algorithm']
        best_pred = predictions[best_model]
        
        ax3.plot(y_test.index, y_test.values, 'o-', label='Real', linewidth=2)
        ax3.plot(y_test.index, best_pred, 's-', label=f'Previsto ({best_model})', 
                linewidth=2, alpha=0.8)
        ax3.set_xlabel('Data')
        ax3.set_ylabel('Valor')
        ax3.set_title(f'Previsões vs Real - {best_model}')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Erros de previsão
        errors = y_test.values - best_pred
        ax4.plot(y_test.index, errors, 'r-', alpha=0.7)
        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        ax4.fill_between(y_test.index, errors, alpha=0.3, color='red')
        ax4.set_xlabel('Data')
        ax4.set_ylabel('Erro (Real - Previsto)')
        ax4.set_title(f'Erros de Previsão - {best_model}')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
    except ImportError:
        print("Matplotlib não disponível para visualização.")
    except Exception as e:
        print(f"Erro na visualização: {e}")


def analyze_feature_importance(comparison_results, top_n=15):
    """
    Analisa a importância das features para modelos que suportam
    """
    
    models = comparison_results['models']
    results = []
    
    for model_name, model in models.items():
        try:
            if model_name in ['XGBoost', 'XGBoost_Linear']:
                # Para XGBoost
                if hasattr(model, 'best_estimator_'):
                    importance = model.best_estimator_.feature_importances_
                    feature_names = comparison_results['data_info']['feature_names']
                else:
                    importance = model.feature_importances_
                    feature_names = comparison_results['data_info']['feature_names']
                
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance,
                    'model': model_name
                }).sort_values('importance', ascending=False)
                
                results.append(importance_df.head(top_n))
                
            elif model_name in ['LightGBM', 'LightGBM_Linear']:
                # Para LightGBM
                if hasattr(model, 'best_estimator_'):
                    importance = model.best_estimator_.feature_importances_
                    feature_names = comparison_results['data_info']['feature_names']
                else:
                    importance = model.feature_importances_
                    feature_names = comparison_results['data_info']['feature_names']
                
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance,
                    'model': model_name
                }).sort_values('importance', ascending=False)
                
                results.append(importance_df.head(top_n))
                
        except Exception as e:
            print(f"Erro ao extrair importância para {model_name}: {e}")
            continue
    
    if results:
        combined_importance = pd.concat(results, ignore_index=True)
        return combined_importance
    else:
        print("Nenhum modelo com importância de features disponível")
        return None


def create_ensemble_prediction(comparison_results, method='weighted_avg', weights=None):
    """
    Cria previsão ensemble baseada nos melhores modelos
    
    Parâmetros:
    -----------
    comparison_results : dict
        Resultados da comparação de modelos
    method : str
        Método de ensemble ('weighted_avg', 'simple_avg', 'best_n')
    weights : dict
        Pesos para cada modelo (apenas para weighted_avg)
    """
    
    predictions = comparison_results['predictions']
    results_df = comparison_results['results']
    y_test = comparison_results['test_data']['y_true']
    
    if method == 'simple_avg':
        # Média simples de todas as previsões
        pred_values = np.array(list(predictions.values()))
        ensemble_pred = np.mean(pred_values, axis=0)
        
    elif method == 'weighted_avg':
        # Média ponderada baseada no RMSE (menor RMSE = maior peso)
        if weights is None:
            # Calcular pesos automaticamente baseado no RMSE inverso
            rmse_values = results_df.set_index('Algorithm')['RMSE']
            weights = {}
            for algo in predictions.keys():
                if algo in rmse_values.index:
                    weights[algo] = 1 / rmse_values[algo]
        
        # Normalizar pesos
        total_weight = sum(weights.values())
        weights = {k: v/total_weight for k, v in weights.items()}
        
        ensemble_pred = np.zeros(len(y_test))
        for algo, pred in predictions.items():
            if algo in weights:
                ensemble_pred += weights[algo] * pred
                
    elif method == 'best_n':
        # Média dos N melhores modelos
        n_best = min(3, len(predictions))
        best_models = results_df.head(n_best)['Algorithm'].tolist()
        
        selected_preds = [predictions[model] for model in best_models if model in predictions]
        ensemble_pred = np.mean(selected_preds, axis=0)
    
    # Calcular métricas do ensemble
    mae = mean_absolute_error(y_test, ensemble_pred)
    mse = mean_squared_error(y_test, ensemble_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_test, ensemble_pred) * 100
    direction_accuracy = _calculate_direction_accuracy(y_test, ensemble_pred)
    
    ensemble_results = {
        'predictions': ensemble_pred,
        'metrics': {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'MAPE': mape,
            'Direction_Accuracy': direction_accuracy,
            'R2_Score': 1 - (mse / np.var(y_test))
        },
        'method': method,
        'weights': weights if method == 'weighted_avg' else None
    }
    
    return ensemble_results


# Exemplo de uso melhorado
if __name__ == "__main__":
    # Criar série temporal de exemplo mais complexa
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=120, freq='M')  # 10 anos de dados mensais
    
    # Série com múltiplos componentes
    t = np.arange(120)
    trend = 100 + 0.5 * t  # Tendência crescente
    seasonal_12 = 15 * np.sin(2 * np.pi * t / 12)  # Sazonalidade anual
    seasonal_3 = 5 * np.sin(2 * np.pi * t / 3)   # Sazonalidade trimestral
    noise = np.random.normal(0, 8, 120)
    
    # Variável target
    sales = trend + seasonal_12 + seasonal_3 + noise
    
    # Variáveis exógenas
    external_factor1 = 50 + 10 * np.sin(2 * np.pi * t / 12) + np.random.normal(0, 5, 120)
    external_factor2 = np.random.normal(30, 8, 120)
    marketing_spend = 100 + 20 * (t / 120) + np.random.normal(0, 15, 120)
    
    ts_data = pd.DataFrame({
        'date': dates,
        'sales': sales,
        'economic_indicator': external_factor1,
        'competitor_price': external_factor2,
        'marketing_spend': marketing_spend
    })
    
    print("=== EXEMPLO APRIMORADO DE USO ===")
    print("Testando algoritmos com busca de hiperparâmetros melhorada...")
    print(f"Dados: {len(ts_data)} observações mensais")
    print(f"Período: {ts_data['date'].min()} até {ts_data['date'].max()}")
    
    # Executar comparação completa
    results = time_series_forecast_comparison(
        df=ts_data,
        target_col='sales',
        date_col='date',
        test_size=0.25,  # 25% para teste (30 meses)
        n_lags=12,       # 12 lags (1 ano)
        external_features=['economic_indicator', 'competitor_price', 'marketing_spend'],
        seasonal_periods=12,  # Sazonalidade anual
        algorithms=['ElasticNet', 'XGBoost', 'XGBoost_Linear', 'LightGBM', 'LightGBM_Linear', 'SARIMAX'],
        cv_folds=5,
        verbose=True
    )
    
    print(f"\n🏆 MELHOR MODELO: {results['best_model']}")
    print("\n📊 RESULTADOS DETALHADOS:")
    print(results['results'].round(4))
    
    # Análise de importância das features
    print("\n🔍 ANÁLISE DE IMPORTÂNCIA DAS FEATURES:")
    importance_analysis = analyze_feature_importance(results, top_n=10)
    if importance_analysis is not None:
        print(importance_analysis)
    
    # Criar ensemble
    print("\n🎯 CRIANDO ENSEMBLE DOS MELHORES MODELOS:")
    ensemble_results = create_ensemble_prediction(results, method='best_n')
    
    print(f"RMSE Ensemble: {ensemble_results['metrics']['RMSE']:.4f}")
    print(f"MAPE Ensemble: {ensemble_results['metrics']['MAPE']:.2f}%")
    print(f"Direction Accuracy Ensemble: {ensemble_results['metrics']['Direction_Accuracy']:.2f}%")
    
    # Comparar ensemble com melhor modelo individual
    best_individual_rmse = results['results'].iloc[0]['RMSE']
    ensemble_rmse = ensemble_results['metrics']['RMSE']
    
    if ensemble_rmse < best_individual_rmse:
        improvement = ((best_individual_rmse - ensemble_rmse) / best_individual_rmse) * 100
        print(f"✅ Ensemble melhorou RMSE em {improvement:.2f}%")
    else:
        degradation = ((ensemble_rmse - best_individual_rmse) / best_individual_rmse) * 100
        print(f"⚠️ Ensemble degradou RMSE em {degradation:.2f}%")
    
    print("\n💡 DICAS:")
    print("- Use plot_forecast_comparison(results) para visualizar os resultados")
    print("- Experimente diferentes métodos de ensemble")
    print("- Ajuste seasonal_periods conforme sua série (12=mensal, 4=trimestral, 52=semanal)")
    print("- Para séries longas, considere aumentar n_lags")
    print("- Modelos lineares (XGBoost_Linear, LightGBM_Linear) podem funcionar melhor para séries mais lineares")
